{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter2_object_detection_ssd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMSshaH2Rb8/5AvNP9rI47S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhlee508/pytorch-advanced/blob/master/chapter2_object_detection_ssd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Git Clone"
      ],
      "metadata": {
        "id": "D_iaVnCRkv-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YutaroOgawa/pytorch_advanced.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0kJ4nzEkxt4",
        "outputId": "ddd19c00-2ff8-49e1-c5e8-1c8fa8737737"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 548, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 548 (delta 34), reused 45 (delta 18), pack-reused 479\u001b[K\n",
            "Receiving objects: 100% (548/548), 17.75 MiB | 20.77 MiB/s, done.\n",
            "Resolving deltas: 100% (293/293), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch_advanced/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFK-77WSk1f-",
        "outputId": "1252d72c-5420-4189-e256-ae41ef16d2a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_advanced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp5sQ7LJk7QQ",
        "outputId": "331f3b06-14bc-4b5c-ef64-665688db96a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_image_classification\t 7_nlp_sentiment_transformer\n",
            "2_objectdetection\t 8_nlp_sentiment_bert\n",
            "3_semantic_segmentation  9_video_classification_eco\n",
            "4_pose_estimation\t etc\n",
            "5_gan_generation\t LICENSE\n",
            "6_gan_anomaly_detection  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 2_objectdetection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSxOS3aYk9Gd",
        "outputId": "239a5770-9a89-41a9-87db-75c04a0ef22c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_advanced/2_objectdetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WgB_aAboasn",
        "outputId": "50222e67-ea7f-4d80-ba00-d24b9e5bc286"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_advanced/2_objectdetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnJH0IQEk_fo",
        "outputId": "3307c666-748f-4a52-efa5-4a28723f616f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-2-3_Dataset_DataLoader.ipynb\t  2-8_SSD_inference.ipynb\n",
            "2-4-5_SSD_model_forward.ipynb\t  data\n",
            "2-6_loss_function.ipynb\t\t  make_folders_and_data_downloads.ipynb\n",
            "2-7_SSD_training.ipynb\t\t  utils\n",
            "2-8_SSD_inference_appendix.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Folders and Data Downloads"
      ],
      "metadata": {
        "id": "nEhxxOVxaav6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "aWbiTtFzdbVx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"data\" 폴더가 존재하지 않는 경우 작성한다\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ],
      "metadata": {
        "id": "7AfVabNedjcP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"weights\" 폴더가 존재하지 않는 경우 작성한다\n",
        "weights_dir = \"./weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)"
      ],
      "metadata": {
        "id": "8HdzLeZHdnCe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VOC2012의 데이터 세트를 여기서 다운로드합니다\n",
        "# 시간이 걸립니다(약 15분)\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    tar = tarfile.TarFile(target_path)  # tar 파일 읽기\n",
        "    tar.extractall(data_dir)  # tar를 압축 해제\n",
        "    tar.close()  # tar 파일 닫기"
      ],
      "metadata": {
        "id": "T4z_xXgzdrZI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 SSD용의 VGG 파라미터를 \"weights\" 폴더에 다운로드\n",
        "# MIT License\n",
        "# Copyright (c) 2017 Max deGroot, Ellis Brown\n",
        "# https://github.com/amdegroot/ssd.pytorch\n",
        "    \n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\"\n",
        "target_path = os.path.join(weights_dir, \"vgg16_reducedfc.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)"
      ],
      "metadata": {
        "id": "tWImUZLtdsti"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 SSD300 모델을 \"weights\" 폴더에 다운로드\n",
        "# MIT License\n",
        "# Copyright (c) 2017 Max deGroot, Ellis Brown\n",
        "# https://github.com/amdegroot/ssd.pytorch\n",
        "\n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\"\n",
        "target_path = os.path.join(weights_dir, \"ssd300_mAP_77.43_v2.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)"
      ],
      "metadata": {
        "id": "_s9gSNZkduA4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(필자가 작성한) 학습된 SSD300 모델 \"ssd300_50.pth\"\n",
        "https://drive.google.com/open?id=1_zTTYQ2j0r-Qe3VBbHzvURD0c1P2ZSE9   \n",
        "    \n",
        "    \n",
        "반드시 다운로드할 필요는 없지만,\n",
        "스스로 SSD 네트워크를 학습하기 전에, 학습된 모델을 미리 시도해보는 경우,\n",
        "위 링크에서 수동으로 다운로드한 \"ssd300_50.pth\" 파일을 \"weights\" 폴더에 배치합니다."
      ],
      "metadata": {
        "id": "s-oyHlNddyug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "※ 미리 실시된 내용\n",
        "\n",
        "수동으로 승마 화상 다운로드\n",
        "\n",
        "https://pixabay.com/ja/photos/%E3%82%AB%E3%82%A6%E3%83%9C%E3%83%BC%E3%82%A4-%E9%A6%AC-%E4%B9%97%E9%A6%AC-%E6%B0%B4-%E6%B5%B7-757575/\n",
        "640×426 크기의 화상을 \"data\" 폴더에 배치\n",
        "(사진의 권리 정보: 상업적 사용 무료, 저작자 표시가 필요하지 않습니다)"
      ],
      "metadata": {
        "id": "EYudCd6yd0B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter2. 물체 감지(SSD)"
      ],
      "metadata": {
        "id": "K--D2PPZZSDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 데이터셋 구현\n",
        "- Dataset 클래스 작성"
      ],
      "metadata": {
        "id": "rn0hSQiAZl_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "gGUpd8r8aPgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "daKErV6aYSYB"
      },
      "outputs": [],
      "source": [
        "# 패키지 import\n",
        "import os.path as osp\n",
        "import random\n",
        "# 파일이나 텍스트에서 XML을 읽고, 가공하고 저장하기 위한 라이브러리\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 난수 시드 설정\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "metadata": {
        "id": "Su_aVtTeaccj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image and Annotation data to List\n",
        "- 화상 데이터, 어노테이션 데이터의 파일 경로 리스트 작성\n",
        "\n"
      ],
      "metadata": {
        "id": "FYZeHwoRiIZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 검증용 화상 데이터, 어노테이션 데이터의 파일 경로 리스트를 작성\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    데이터의 경로를 저장한 리스트를 작성한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        데이터 폴더의 경로\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        데이터의 경로를 저장한 리스트\n",
        "    \"\"\"\n",
        "\n",
        "    # 화상 파일과 어노테이션 파일의 경로 템플릿을 작성\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "\n",
        "    # 훈련 및 검증 각각의 파일 ID(파일 이름)를 취득\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "\n",
        "    # 훈련 데이터의 화상 파일과 어노테이션 파일의 경로 리스트를 작성\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 공백과 줄바꿈 제거\n",
        "        img_path = (imgpath_template % file_id)  # 화상의 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션의 경로\n",
        "        train_img_list.append(img_path)  # 리스트에 추가\n",
        "        train_anno_list.append(anno_path)  # 리스트에 추가\n",
        "\n",
        "    # 검증 데이터의 화상 파일과 어노테이션 파일의 경로 리스트 작성\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 공백과 줄바꿈 제거\n",
        "        img_path = (imgpath_template % file_id)  # 화상의 경로\n",
        "        anno_path = (annopath_template % file_id)  # 어노테이션의 경로\n",
        "        val_img_list.append(img_path)  # 리스트에 추가\n",
        "        val_anno_list.append(anno_path)  # 리스트에 추가\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
      ],
      "metadata": {
        "id": "ltXXm8SKabEQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 경로 리스트 작성\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
        "\n",
        "# 동작 확인\n",
        "print(train_img_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4YK_bZ2cQ1n",
        "outputId": "0ff5eb91-1a7a-456b-9681-83c7274ef1e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/VOCdevkit/VOC2012/JPEGImages/2008_000008.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XML Annotation to List\n",
        "- xml 형식의 어노테이션 데이터를 리스트로 변환하기"
      ],
      "metadata": {
        "id": "ZppzxPB5klUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"XML 형식의 어노테이션\"을 리스트 형식으로 변환하는 클래스\n",
        "class Anno_xml2list(object):\n",
        "    \"\"\"\n",
        "    한 장의 이미지에 대한 \"XML 형식의 어노테이션 데이터\"를 화상 크기로 규격화해 리스트 형식으로 변환한다.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    classes : 리스트\n",
        "        VOC의 클래스명을 저장한 리스트\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        \"\"\"\n",
        "        한 장의 이미지에 대한 \"XML 형식의 어노테이션 데이터\"를 화상 크기로 규격화해 리스트 형식으로 변환한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xml_path : str\n",
        "            xml 파일의 경로.\n",
        "        width : int\n",
        "            대상 화상의 폭.\n",
        "        height : int\n",
        "            대상 화상의 높이.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "            물체의 어노테이션 데이터를 저장한 리스트. 이미지에 존재하는 물체수만큼의 요소를 가진다.\n",
        "        \"\"\"\n",
        "\n",
        "        # 화상 내 모든 물체의 어노테이션을 이 리스트에 저장합니다\n",
        "        ret = []\n",
        "\n",
        "        # xml 파일을 로드\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        # 화상 내에 있는 물체(object)의 수만큼 반복\n",
        "        for obj in xml.iter('object'):\n",
        "\n",
        "            # 어노테이션에서 감지가 difficult로 설정된 것은 제외\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "\n",
        "            # 한 물체의 어노테이션을 저장하는 리스트\n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()  # 물체 이름\n",
        "            bbox = obj.find('bndbox')  # 바운딩 박스 정보\n",
        "\n",
        "            # 어노테이션의 xmin, ymin, xmax, ymax를 취득하고, 0~1으로 규격화\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                # VOC는 원점이 (1,1)이므로 1을 빼서 (0, 0)으로 한다\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1\n",
        "\n",
        "                # 폭, 높이로 규격화\n",
        "                if pt == 'xmin' or pt == 'xmax':  # x 방향의 경우 폭으로 나눈다\n",
        "                    cur_pixel /= width\n",
        "                else:  # y 방향의 경우 높이로 나눈다\n",
        "                    cur_pixel /= height\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            # 어노테이션의 클래스명 index를 취득하여 추가\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            # res에 [xmin, ymin, xmax, ymax, label_ind]을 더한다\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]"
      ],
      "metadata": {
        "id": "21HtDNlBkwLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동작 확인\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "\n",
        "# 화상 로드용으로 OpenCV를 사용\n",
        "ind = 1\n",
        "image_file_path = val_img_list[ind]\n",
        "img = cv2.imread(image_file_path)  # [높이][폭][색BGR]\n",
        "height, width, channels = img.shape  # 화상 크기 취득\n",
        "\n",
        "# 어노테이션을 리스트로 표시\n",
        "transform_anno(val_anno_list[ind], width, height)"
      ],
      "metadata": {
        "id": "iNpiS_74lEsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}